{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq to Seq from Scratch using Tensorflow\n",
    "#### Author: Subhojeet Pramanik\n",
    "\n",
    "\n",
    "\n",
    "#### Task:\n",
    "To write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "2 27 1 0\n",
      "  y #\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 2 # [a-z] + ' ' + #(end of sentence)\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 2\n",
    "  elif char == ' ':\n",
    "    return 1\n",
    "  elif char=='#':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 1:\n",
    "        return chr(dictid + first_letter - 2)\n",
    "    elif dictid==1:\n",
    "        return ' '\n",
    "    elif dictid==0:\n",
    "        return '#'\n",
    "\n",
    "\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reverse(alist):\n",
    "    newlist = []\n",
    "    for i in range(1, len(alist) + 1):\n",
    "        newlist.append(alist[-i])\n",
    "    return newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ns anarchists#', 'hen military #', 'leria arches #', 'abbeys and mo#', 'arried urraca#', 'el and richar#', ' and liturgic#', 'y opened for #', 'ion from the #', 'igration took#', 'ew york other#', 'e boeing seve#', ' listed with #', 'ber has proba#', ' be made to r#', 'er who receiv#', 're significan#', ' fierce criti#', 'two six eight#', 'ristotle s un#', 'ty can be los#', 'and intracell#', 'ion of the si#', 'y to pass him#', ' certain drug#', 't it will tak#', ' convince the#', 'nt told him t#', 'mpaign and ba#', 'ver side stan#', 'ous texts suc#', ' capitalize o#', ' duplicate of#', 'h ann es d hi#', 'ne january ei#', 'oss zero the #', 'al theories c#', 'st instance t#', 'dimensional a#', 'ost holy morm#', ' s support or#', ' is still dis#', ' oscillating #', ' eight subtyp#', 'f italy langu#', ' the tower co#', 'lahoma press #', 'rprise linux #', 's becomes the#', 't in a nazi c#', 'he fabian soc#', 'tchy to relat#', 'sharman netwo#', 'sed emperor h#', 'ing in politi#', ' neo latin mo#', 'h risky riske#', 'ncyclopedic o#', 'ense the air #', 'uating from a#', 'reet grid cen#', 'tions more th#', 'ppeal of devo#', 'i have made s#']\n",
      "['sn stsihcrana#', 'neh yratilim #', 'airel sehcra #', 'syebba dna om#', 'deirra acarru#', 'le dna rahcir#', ' dna cigrutil#', 'y denepo rof #', 'noi morf eht #', 'noitargi koot#', 'we kroy rehto#', 'e gnieob eves#', ' detsil htiw #', 'reb sah aborp#', ' eb edam ot r#', 're ohw viecer#', 'er nacifingis#', ' ecreif itirc#', 'owt xis thgie#', 'eltotsir s nu#', 'yt nac eb sol#', 'dna llecartni#', 'noi fo eht is#', 'y ot ssap mih#', ' niatrec gurd#', 't ti lliw kat#', ' ecnivnoc eht#', 'tn dlot mih t#', 'ngiapm dna ab#', 'rev edis nats#', 'suo stxet cus#', ' ezilatipac o#', ' etacilpud fo#', 'h nna se d ih#', 'en yraunaj ie#', 'sso orez eht #', 'la seiroeht c#', 'ts ecnatsni t#', 'lanoisnemid a#', 'tso yloh mrom#', ' s troppus ro#', ' si llits sid#', ' gnitallicso #', ' thgie pytbus#', 'f ylati ugnal#', ' eht rewot oc#', 'amohal sserp #', 'esirpr xunil #', 's semoceb eht#', 't ni a izan c#', 'eh naibaf cos#', 'yhct ot taler#', 'namrahs owten#', 'des rorepme h#', 'gni ni itilop#', ' oen nital om#', 'h yksir eksir#', 'cidepolcycn o#', 'esne eht ria #', 'gnitau morf a#', 'teer dirg nec#', 'snoit erom ht#', 'laepp fo oved#', 'i evah edam s#']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=14\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    batches=[]\n",
    "    for step in range(self._num_unrollings-1):\n",
    "      batches.append(self._next_batch())\n",
    "    #The EOS character for each batch\n",
    "    \n",
    "    #Generating the output batches by reversing each word in a num_unrolling size sentence\n",
    "    output_batches=[]\n",
    "    for step in range(self._num_unrollings-1):\n",
    "        output_batches.append(np.zeros(shape=(self._batch_size, vocabulary_size),dtype=np.float))\n",
    "    for b in range(self._batch_size):\n",
    "        words=[]\n",
    "        #Will store each of characters for words, is emptied when a space is encountered\n",
    "        array=[]\n",
    "        for i in range(self._num_unrollings-1):\n",
    "            if(np.argmax(batches[i][b,:])!=1):\n",
    "                array.append(np.argmax(batches[i][b,:]))\n",
    "            else:\n",
    "                array=reverse(array)\n",
    "                words.extend(array)\n",
    "                words.append(1)\n",
    "                array=[]\n",
    "        array=reverse(array)\n",
    "        words.extend(array)\n",
    "        for i in range(self._num_unrollings-1):\n",
    "            output_batches[i][b,words[i]]=1\n",
    "        \n",
    "    last_batch=np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    #Set the last batch character to EOS for the last batch\n",
    "    \n",
    "    last_batch[:,0]=1\n",
    "    batches.append(last_batch)\n",
    "    output_batches.append(last_batch)\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches,output_batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, num_unrollings)\n",
    "batches,output_batches=train_batches.next()\n",
    "print(batches2string(batches))\n",
    "print(batches2string(output_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split every 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "\n",
    "def accuracy(labels,predictions):\n",
    "    return np.sum(np.argmax(labels,axis=1)==np.argmax(predictions,axis=1))/labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Decoder Architecture\n",
    "\n",
    "First we will implement a simple encoder-decoder architecture as specified in (http://arxiv.org/abs/1409.3215)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes =256\n",
    "#dropout=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class lstm:\n",
    "    def __init__(self,input_size):\n",
    "        self.xx = tf.Variable(tf.truncated_normal([input_size, num_nodes * 4], -0.1, 0.1))\n",
    "        self.mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "        self.bb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "\n",
    "\n",
    "                          \n",
    "    def lstm_cell(self,i,o,state):\n",
    "        global dropout\n",
    "        #i=tf.nn.dropout(i,keep_prob=dropout)\n",
    "        matmuls = tf.matmul(i, self.xx)+ tf.matmul(o, self.mm) + self.bb        \n",
    "        input_gate  = tf.sigmoid(matmuls[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmuls[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update      =            matmuls[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmuls[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output=output_gate * tf.tanh(state)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    #LSTM for encoder and decoder\n",
    "    encoder_lstm=lstm(vocabulary_size)\n",
    "    decoder_lstm=lstm(vocabulary_size)\n",
    "    #The Encoder function\n",
    "    def encoder(train_input,output,state):\n",
    "        '''\n",
    "        Args:       \n",
    "        \n",
    "        train_input : array of size num_unrolling, each array element is a Tensor of dimension batch_size,\n",
    "        vocabulary size.\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        output : Output of LSTM aka Hidden State\n",
    "        state : Cell state of the LSTM\n",
    "        \n",
    "        '''\n",
    "        i = len(train_inputs) - 1\n",
    "        while i >= 0:\n",
    "            output, state = encoder_lstm.lstm_cell(train_input[i],output,state)\n",
    "            i=i-1\n",
    "        #Return the last output of the lstm cell for decoding\n",
    "        return output ,state\n",
    "    \n",
    "  \n",
    "\n",
    "    def training_decoder(decoder_input,output,state):\n",
    "        outputs=[]\n",
    "        #Predict the first character using the EOS Tag. We use EOS tag as the start tag\n",
    "        output, state = decoder_lstm.lstm_cell(decoder_input[-1],output,state)\n",
    "        outputs.append(output)\n",
    "        #Now predict the next outputs using the training labels itself. Using y(n-1) to predict y(n)\n",
    "        for i in decoder_input[0:-1]:\n",
    "            output,state=decoder_lstm.lstm_cell(i,output,state)\n",
    "            outputs.append(output)\n",
    "        return outputs \n",
    "    \n",
    "    \n",
    "    def inference_decoder(go_char,decode_steps,output,state):\n",
    "        outputs=[]\n",
    "        #First input to decoder is the the Go Character\n",
    "        output,state=decoder_lstm.lstm_cell(go_char,output,state)\n",
    "        outputs.append(output)\n",
    "        for i in range(decode_steps-1):\n",
    "            #Feed the previous output as the next decoder input\n",
    "            decoder_input=tf.nn.softmax(tf.nn.xw_plus_b(output, w, b))\n",
    "            output,state=decoder_lstm.lstm_cell(decoder_input,output,state)\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "    \n",
    "    #State saving across unrollings\n",
    "    saved_state=tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_output=tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) \n",
    "    state=saved_state\n",
    "    output=saved_output\n",
    "    \n",
    "    \n",
    "    reset_state = tf.group(\n",
    "        output.assign(tf.zeros([batch_size, num_nodes])),\n",
    "        state.assign(tf.zeros([batch_size, num_nodes])),\n",
    "        )\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    #Define our train input, decoder output variables\n",
    "    train_inputs=[]\n",
    "    decoder_inputs=[]\n",
    "    outputs=[]\n",
    "    \n",
    "    for i in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.float32,shape=[batch_size,vocabulary_size]))\n",
    "        decoder_inputs.append(tf.placeholder(tf.float32,shape=[batch_size,vocabulary_size]))\n",
    "        \n",
    "    \n",
    "    #Model Definition\n",
    "\n",
    "    output,state=encoder(train_inputs,output,state)\n",
    "\n",
    "    outputs=training_decoder(decoder_inputs,output,state)\n",
    "    \n",
    "\n",
    "\n",
    "    with tf.control_dependencies([saved_state.assign(state),\n",
    "                                saved_output.assign(output),\n",
    "                                    ]):\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                    labels=tf.concat(decoder_inputs, 0), logits=logits))\n",
    "        \n",
    "    #Loss function and optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                            10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "                    zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Sample Prediction\n",
    "    sample_input=[]\n",
    "    sample_outputs=[]\n",
    "\n",
    "    for i in range(num_unrollings):\n",
    "        sample_input.append(tf.placeholder(tf.float32,shape=[1,vocabulary_size]))\n",
    "        \n",
    "    sample_saved_state=tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "    sample_saved_output=tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "    \n",
    "\n",
    "    \n",
    "    sample_output=sample_saved_output\n",
    "    sample_state=sample_saved_state\n",
    "\n",
    "    \n",
    "    \n",
    "    reset_sample_state = tf.group(\n",
    "        sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        sample_state.assign(tf.zeros([1, num_nodes])),\n",
    "\n",
    "        )\n",
    "    \n",
    "\n",
    "    sample_output,sample_state=encoder(sample_input,sample_output,sample_state)\n",
    "    sample_decoder_outputs=inference_decoder(sample_input[-1],num_unrollings,sample_output,sample_state)\n",
    "\n",
    "    with tf.control_dependencies([sample_saved_output.assign(sample_output),\n",
    "                                sample_saved_state.assign(sample_state),\n",
    "                               ]):\n",
    "        for d in sample_decoder_outputs:\n",
    "                sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(d, w, b))\n",
    "                sample_outputs.append(sample_prediction)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "--------------------------------------------------------------------------------\n",
      "Step 0\n",
      "Loss 3.34675\n",
      "Batch Accuracy: 0.89\n",
      "Validation Accuracy: 3.12\n",
      "Input Test String ['nt means to d#']\n",
      "Output Prediction['fazsnzsnzsnzsn']\n",
      "Actual['tn snaem ot d#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 1000\n",
      "Loss 2.21325\n",
      "Batch Accuracy: 33.93\n",
      "Validation Accuracy: 16.80\n",
      "Input Test String ['he term is st#']\n",
      "Output Prediction['  ee      eep#']\n",
      "Actual['eh mret si ts#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 2000\n",
      "Loss 1.86579\n",
      "Batch Accuracy: 44.98\n",
      "Validation Accuracy: 27.06\n",
      "Input Test String ['he english re#']\n",
      "Output Prediction['ee eeee e e s#']\n",
      "Actual['eh hsilgne er#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 3000\n",
      "Loss 1.52316\n",
      "Batch Accuracy: 54.58\n",
      "Validation Accuracy: 37.02\n",
      "Input Test String ['se first used#']\n",
      "Output Prediction['en serol raat#']\n",
      "Actual['es tsrif desu#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 4000\n",
      "Loss 1.2771\n",
      "Batch Accuracy: 61.05\n",
      "Validation Accuracy: 42.66\n",
      "Input Test String ['uctures and c#']\n",
      "Output Prediction['seiruum nnm a#']\n",
      "Actual['serutcu dna c#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 5000\n",
      "Loss 1.04514\n",
      "Batch Accuracy: 67.30\n",
      "Validation Accuracy: 44.87\n",
      "Input Test String ['ritarian soci#']\n",
      "Output Prediction['niitiram naab#']\n",
      "Actual['nairatir icos#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 6000\n",
      "Loss 0.688756\n",
      "Batch Accuracy: 79.69\n",
      "Validation Accuracy: 53.72\n",
      "Input Test String ['it does not i#']\n",
      "Output Prediction['ti seoo ton t#']\n",
      "Actual['ti seod ton i#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 7000\n",
      "Loss 0.452363\n",
      "Batch Accuracy: 87.05\n",
      "Validation Accuracy: 58.45\n",
      "Input Test String [' institutions#']\n",
      "Output Prediction[' sitititnnnu #']\n",
      "Actual[' snoitutitsni#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 8000\n",
      "Loss 0.362716\n",
      "Batch Accuracy: 88.06\n",
      "Validation Accuracy: 66.70\n",
      "Input Test String ['rs to related#']\n",
      "Output Prediction['sr ot detaera#']\n",
      "Actual['sr ot detaler#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 9000\n",
      "Loss 0.169325\n",
      "Batch Accuracy: 95.09\n",
      "Validation Accuracy: 77.77\n",
      "Input Test String ['gh there are #']\n",
      "Output Prediction['hg ereht era #']\n",
      "Actual['hg ereht era #']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 10000\n",
      "Loss 0.191884\n",
      "Batch Accuracy: 94.20\n",
      "Validation Accuracy: 81.19\n",
      "Input Test String ['ophy is the b#']\n",
      "Output Prediction['yhpo si eht b#']\n",
      "Actual['yhpo si eht b#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 11000\n",
      "Loss 0.0953174\n",
      "Batch Accuracy: 97.21\n",
      "Validation Accuracy: 85.31\n",
      "Input Test String ['om the greek #']\n",
      "Output Prediction['mo eht keerg #']\n",
      "Actual['mo eht keerg #']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 12000\n",
      "Loss 0.0970223\n",
      "Batch Accuracy: 97.32\n",
      "Validation Accuracy: 89.54\n",
      "Input Test String [' a positive l#']\n",
      "Output Prediction[' a evitsops i#']\n",
      "Actual[' a evitisop l#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 13000\n",
      "Loss 0.0898283\n",
      "Batch Accuracy: 97.54\n",
      "Validation Accuracy: 92.35\n",
      "Input Test String ['ent means to #']\n",
      "Output Prediction['tne snaem ot #']\n",
      "Actual['tne snaem ot #']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 14000\n",
      "Loss 0.0493461\n",
      "Batch Accuracy: 98.55\n",
      "Validation Accuracy: 92.35\n",
      "Input Test String ['the term is s#']\n",
      "Output Prediction['eht mret si s#']\n",
      "Actual['eht mret si s#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 15000\n",
      "Loss 0.0410769\n",
      "Batch Accuracy: 99.11\n",
      "Validation Accuracy: 92.25\n",
      "Input Test String ['the english r#']\n",
      "Output Prediction['eht hsilgne r#']\n",
      "Actual['eht hsilgne r#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 16000\n",
      "Loss 0.0619939\n",
      "Batch Accuracy: 98.10\n",
      "Validation Accuracy: 95.98\n",
      "Input Test String ['use first use#']\n",
      "Output Prediction['esu tsrif esu#']\n",
      "Actual['esu tsrif esu#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 17000\n",
      "Loss 0.0358551\n",
      "Batch Accuracy: 99.33\n",
      "Validation Accuracy: 95.67\n",
      "Input Test String ['ructures and #']\n",
      "Output Prediction['serutcur dna #']\n",
      "Actual['serutcur dna #']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 18000\n",
      "Loss 0.0191378\n",
      "Batch Accuracy: 99.67\n",
      "Validation Accuracy: 97.18\n",
      "Input Test String ['oritarian soc#']\n",
      "Output Prediction['nairatiro cos#']\n",
      "Actual['nairatiro cos#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 19000\n",
      "Loss 0.0232651\n",
      "Batch Accuracy: 99.22\n",
      "Validation Accuracy: 96.38\n",
      "Input Test String [' it does not #']\n",
      "Output Prediction[' ti seod ton #']\n",
      "Actual[' ti seod ton #']\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20000\n",
    "summary_frequency = 1000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  global dropout\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches,output_batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    dropout=0.5\n",
    "    \n",
    "    for i in range(num_unrollings):\n",
    "        #Feeding input from reverse according to https://arxiv.org/abs/1409.3215\n",
    "        feed_dict[train_inputs[i]]=batches[i]\n",
    "        feed_dict[decoder_inputs[i]]=output_batches[i]\n",
    "\n",
    "        \n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    #reset_state.run()\n",
    "\n",
    "    if step % (summary_frequency ) == 0:\n",
    "        dropout=1\n",
    "        print('-'*80)\n",
    "        print('Step '+str(step))\n",
    "        print('Loss '+str(l))\n",
    "        \n",
    "        labels=np.concatenate(list(output_batches)[:])\n",
    "#        print(characters(labels))\n",
    "#       print(characters(predictions))\n",
    "        print('Batch Accuracy: %.2f' % float(accuracy(labels,predictions)*100))\n",
    "        num_validation = valid_size // num_unrollings\n",
    "        reset_sample_state.run()\n",
    "        sum_acc=0\n",
    "        for _ in range(num_validation):\n",
    "            valid,valid_output=valid_batches.next()\n",
    "            valid_feed_dict=dict()\n",
    "            for i in range(num_unrollings):\n",
    "                valid_feed_dict[sample_input[i]]=valid[i]\n",
    "            sample_pred=session.run(sample_outputs,feed_dict=valid_feed_dict)\n",
    "            labels=np.concatenate(list(valid_output)[:],axis=0)\n",
    "            pred=np.concatenate(list(sample_pred)[:],axis=0)\n",
    "            sum_acc=sum_acc+accuracy(labels,pred)\n",
    "        val_acc=sum_acc/num_validation\n",
    "        print('Validation Accuracy: %0.2f'%(val_acc*100))\n",
    "        print('Input Test String '+str(batches2string(valid)))\n",
    "        print('Output Prediction'+str(batches2string(sample_pred)))\n",
    "        print('Actual'+str(batches2string(valid_output)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism\n",
    "\n",
    "Normal Encoder Decoder architectures only look  at the fixed vector representation as provided by the Encoder. Adding an attention mechanism helps the decoder to focus on the hidden states produced at different timestep by the encoder, and only pay attention to those that are relevant at a particular decoding step.\n",
    "\n",
    "This notebook implements a soft attention. The implementation has been done as per the below diagram. Refer to https://arxiv.org/pdf/1409.0473.pdf for more details.\n",
    "\n",
    "![alt text](https://heuritech.files.wordpress.com/2016/01/detail_attentionmodel_dotproduct1.png \"Attention Mechanism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 2 but is rank 3 for 'MatMul_29' (op: 'MatMul') with input shapes: [14,64,256], [256,1024].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    672\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 2 but is rank 3 for 'MatMul_29' (op: 'MatMul') with input shapes: [14,64,256], [256,1024].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-654bd021350a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-654bd021350a>\u001b[0m in \u001b[0;36mtraining_decoder\u001b[0;34m(decoder_input, output, state)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m#Predict the first character using the EOS Tag. We use EOS tag as the start tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#Now predict the next outputs using the training labels itself. Using y(n-1) to predict y(n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-045e207a5d74>\u001b[0m in \u001b[0;36mlstm_cell\u001b[0;34m(self, i, o, state)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#i=tf.nn.dropout(i,keep_prob=dropout)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mmatmuls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0minput_gate\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatmuls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_nodes\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mforget_gate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatmuls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_nodes\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1814\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[0;32m-> 1816\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_mat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   1215\u001b[0m   \"\"\"\n\u001b[1;32m   1216\u001b[0m   result = _op_def_lib.apply_op(\"MatMul\", a=a, b=b, transpose_a=transpose_a,\n\u001b[0;32m-> 1217\u001b[0;31m                                 transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1218\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2506\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2508\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2509\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1871\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1873\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1874\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 2 but is rank 3 for 'MatMul_29' (op: 'MatMul') with input shapes: [14,64,256], [256,1024]."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #Attention weights\n",
    "    attn_weights=[]\n",
    "    for _ in range(num_unrollings):\n",
    "        attn_weights.append(tf.Variable(tf.truncated_normal([num_nodes*2, num_nodes], -0.1, 0.1)))\n",
    "        \n",
    "    #LSTM for encoder and decoder\n",
    "    encoder_lstm=lstm(vocabulary_size)\n",
    "    decoder_lstm=lstm(vocabulary_size)\n",
    "    #The Encoder function\n",
    "    def encoder(train_input,output,state):\n",
    "        '''\n",
    "        Args:       \n",
    "        \n",
    "        train_input : array of size num_unrolling, each array element is a Tensor of dimension batch_size,\n",
    "        vocabulary size.\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        output : Output of LSTM aka Hidden State\n",
    "        state : Cell state of the LSTM\n",
    "        \n",
    "        '''\n",
    "        i = len(train_inputs) - 1\n",
    "        outputs=[]\n",
    "        while i >= 0:\n",
    "            output, state = encoder_lstm.lstm_cell(train_input[i],output,state)\n",
    "            outputs.append(output)\n",
    "            i=i-1\n",
    "        #Return the last output of the lstm cell for decoding\n",
    "        return outputs ,state\n",
    "    \n",
    "    def soft_attention():\n",
    "        '''\n",
    "        Implements soft attention mechanism over an array of hidden states\n",
    "        '''\n",
    "        \n",
    "    \n",
    "    def training_decoder(decoder_input,output,state):\n",
    "        outputs=[]\n",
    "        #Predict the first character using the EOS Tag. We use EOS tag as the start tag\n",
    "        output, state = decoder_lstm.lstm_cell(decoder_input[-1],output,state)\n",
    "        outputs.append(output)\n",
    "        #Now predict the next outputs using the training labels itself. Using y(n-1) to predict y(n)\n",
    "        for i in decoder_input[0:-1]:\n",
    "            output,state=decoder_lstm.lstm_cell(i,output,state)\n",
    "            outputs.append(output)\n",
    "        return outputs \n",
    "    \n",
    "    \n",
    "    def inference_decoder(go_char,decode_steps,output,state):\n",
    "        outputs=[]\n",
    "        #First input to decoder is the the Go Character\n",
    "        output,state=decoder_lstm.lstm_cell(go_char,output,state)\n",
    "        outputs.append(output)\n",
    "        for i in range(decode_steps-1):\n",
    "            #Feed the previous output as the next decoder input\n",
    "            decoder_input=tf.nn.softmax(tf.nn.xw_plus_b(output, w, b))\n",
    "            output,state=decoder_lstm.lstm_cell(decoder_input,output,state)\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "    \n",
    "    #State saving across unrollings\n",
    "    saved_state=tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_output=tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) \n",
    "    state=saved_state\n",
    "    output=saved_output\n",
    "    \n",
    "    \n",
    "    reset_state = tf.group(\n",
    "        output.assign(tf.zeros([batch_size, num_nodes])),\n",
    "        state.assign(tf.zeros([batch_size, num_nodes])),\n",
    "        )\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    #Define our train input, decoder output variables\n",
    "    train_inputs=[]\n",
    "    decoder_inputs=[]\n",
    "    outputs=[]\n",
    "    \n",
    "    for i in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.float32,shape=[batch_size,vocabulary_size]))\n",
    "        decoder_inputs.append(tf.placeholder(tf.float32,shape=[batch_size,vocabulary_size]))\n",
    "        \n",
    "    \n",
    "    #Model Definition\n",
    "\n",
    "    output,state=encoder(train_inputs,output,state)\n",
    "\n",
    "    outputs=training_decoder(decoder_inputs,output,state)\n",
    "    \n",
    "\n",
    "\n",
    "    with tf.control_dependencies([saved_state.assign(state),\n",
    "                                saved_output.assign(output),\n",
    "                                    ]):\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                    labels=tf.concat(decoder_inputs, 0), logits=logits))\n",
    "        \n",
    "    #Loss function and optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                            10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "                    zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Sample Prediction\n",
    "    sample_input=[]\n",
    "    sample_outputs=[]\n",
    "\n",
    "    for i in range(num_unrollings):\n",
    "        sample_input.append(tf.placeholder(tf.float32,shape=[1,vocabulary_size]))\n",
    "        \n",
    "    sample_saved_state=tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "    sample_saved_output=tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "    \n",
    "\n",
    "    \n",
    "    sample_output=sample_saved_output\n",
    "    sample_state=sample_saved_state\n",
    "\n",
    "    \n",
    "    \n",
    "    reset_sample_state = tf.group(\n",
    "        sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        sample_state.assign(tf.zeros([1, num_nodes])),\n",
    "\n",
    "        )\n",
    "    \n",
    "\n",
    "    sample_output,sample_state=encoder(sample_input,sample_output,sample_state)\n",
    "    sample_decoder_outputs=inference_decoder(sample_input[-1],num_unrollings,sample_output,sample_state)\n",
    "\n",
    "    with tf.control_dependencies([sample_saved_output.assign(sample_output),\n",
    "                                sample_saved_state.assign(sample_state),\n",
    "                               ]):\n",
    "        for d in sample_decoder_outputs:\n",
    "                sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(d, w, b))\n",
    "                sample_outputs.append(sample_prediction)\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
