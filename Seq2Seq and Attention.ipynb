{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq to Seq from Scratch using Tensorflow\n",
    "#### Author: Subhojeet Pramanik\n",
    "\n",
    "\n",
    "\n",
    "#### Task:\n",
    "To write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "2 27 1 0\n",
      "  y #\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 2 # [a-z] + ' ' + #(end of sentence)\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 2\n",
    "  elif char == ' ':\n",
    "    return 1\n",
    "  elif char=='#':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 1:\n",
    "        return chr(dictid + first_letter - 2)\n",
    "    elif dictid==1:\n",
    "        return ' '\n",
    "    elif dictid==0:\n",
    "        return '#'\n",
    "\n",
    "\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reverse(alist):\n",
    "    newlist = []\n",
    "    for i in range(1, len(alist) + 1):\n",
    "        newlist.append(alist[-i])\n",
    "    return newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ns anarchists#', 'hen military #', 'leria arches #', 'abbeys and mo#', 'arried urraca#', 'el and richar#', ' and liturgic#', 'y opened for #', 'ion from the #', 'igration took#', 'ew york other#', 'e boeing seve#', ' listed with #', 'ber has proba#', ' be made to r#', 'er who receiv#', 're significan#', ' fierce criti#', 'two six eight#', 'ristotle s un#', 'ty can be los#', 'and intracell#', 'ion of the si#', 'y to pass him#', ' certain drug#', 't it will tak#', ' convince the#', 'nt told him t#', 'mpaign and ba#', 'ver side stan#', 'ous texts suc#', ' capitalize o#', ' duplicate of#', 'h ann es d hi#', 'ne january ei#', 'oss zero the #', 'al theories c#', 'st instance t#', 'dimensional a#', 'ost holy morm#', ' s support or#', ' is still dis#', ' oscillating #', ' eight subtyp#', 'f italy langu#', ' the tower co#', 'lahoma press #', 'rprise linux #', 's becomes the#', 't in a nazi c#', 'he fabian soc#', 'tchy to relat#', 'sharman netwo#', 'sed emperor h#', 'ing in politi#', ' neo latin mo#', 'h risky riske#', 'ncyclopedic o#', 'ense the air #', 'uating from a#', 'reet grid cen#', 'tions more th#', 'ppeal of devo#', 'i have made s#']\n",
      "['sn stsihcrana#', 'neh yratilim #', 'airel sehcra #', 'syebba dna om#', 'deirra acarru#', 'le dna rahcir#', ' dna cigrutil#', 'y denepo rof #', 'noi morf eht #', 'noitargi koot#', 'we kroy rehto#', 'e gnieob eves#', ' detsil htiw #', 'reb sah aborp#', ' eb edam ot r#', 're ohw viecer#', 'er nacifingis#', ' ecreif itirc#', 'owt xis thgie#', 'eltotsir s nu#', 'yt nac eb sol#', 'dna llecartni#', 'noi fo eht is#', 'y ot ssap mih#', ' niatrec gurd#', 't ti lliw kat#', ' ecnivnoc eht#', 'tn dlot mih t#', 'ngiapm dna ab#', 'rev edis nats#', 'suo stxet cus#', ' ezilatipac o#', ' etacilpud fo#', 'h nna se d ih#', 'en yraunaj ie#', 'sso orez eht #', 'la seiroeht c#', 'ts ecnatsni t#', 'lanoisnemid a#', 'tso yloh mrom#', ' s troppus ro#', ' si llits sid#', ' gnitallicso #', ' thgie pytbus#', 'f ylati ugnal#', ' eht rewot oc#', 'amohal sserp #', 'esirpr xunil #', 's semoceb eht#', 't ni a izan c#', 'eh naibaf cos#', 'yhct ot taler#', 'namrahs owten#', 'des rorepme h#', 'gni ni itilop#', ' oen nital om#', 'h yksir eksir#', 'cidepolcycn o#', 'esne eht ria #', 'gnitau morf a#', 'teer dirg nec#', 'snoit erom ht#', 'laepp fo oved#', 'i evah edam s#']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=14\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    batches=[]\n",
    "    for step in range(self._num_unrollings-1):\n",
    "      batches.append(self._next_batch())\n",
    "    #The EOS character for each batch\n",
    "    \n",
    "    #Generating the output batches by reversing each word in a num_unrolling size sentence\n",
    "    output_batches=[]\n",
    "    for step in range(self._num_unrollings-1):\n",
    "        output_batches.append(np.zeros(shape=(self._batch_size, vocabulary_size),dtype=np.float))\n",
    "    for b in range(self._batch_size):\n",
    "        words=[]\n",
    "        #Will store each of characters for words, is emptied when a space is encountered\n",
    "        array=[]\n",
    "        for i in range(self._num_unrollings-1):\n",
    "            if(np.argmax(batches[i][b,:])!=1):\n",
    "                array.append(np.argmax(batches[i][b,:]))\n",
    "            else:\n",
    "                array=reverse(array)\n",
    "                words.extend(array)\n",
    "                words.append(1)\n",
    "                array=[]\n",
    "        array=reverse(array)\n",
    "        words.extend(array)\n",
    "        for i in range(self._num_unrollings-1):\n",
    "            output_batches[i][b,words[i]]=1\n",
    "        \n",
    "    last_batch=np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    #Set the last batch character to EOS for the last batch\n",
    "    \n",
    "    last_batch[:,0]=1\n",
    "    batches.append(last_batch)\n",
    "    output_batches.append(last_batch)\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches,output_batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, num_unrollings)\n",
    "batches,output_batches=train_batches.next()\n",
    "print(batches2string(batches))\n",
    "print(batches2string(output_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split every 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "\n",
    "def accuracy(labels,predictions):\n",
    "    return np.sum(np.argmax(labels,axis=1)==np.argmax(predictions,axis=1))/labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Decoder Architecture\n",
    "\n",
    "First we will implement a simple encoder-decoder architecture as specified in (http://arxiv.org/abs/1409.3215)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes =256\n",
    "#dropout=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class lstm:\n",
    "    def __init__(self,input_size):\n",
    "        self.xx = tf.Variable(tf.truncated_normal([input_size, num_nodes * 4], -0.1, 0.1))\n",
    "        self.mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "        self.bb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "\n",
    "\n",
    "                          \n",
    "    def lstm_cell(self,i,o,state):\n",
    "        global dropout\n",
    "        #i=tf.nn.dropout(i,keep_prob=dropout)\n",
    "        matmuls = tf.matmul(i, self.xx)+ tf.matmul(o, self.mm) + self.bb        \n",
    "        input_gate  = tf.sigmoid(matmuls[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmuls[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update      =            matmuls[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmuls[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output=output_gate * tf.tanh(state)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    #LSTM for encoder and decoder\n",
    "    encoder_lstm=lstm(vocabulary_size)\n",
    "    decoder_lstm=lstm(vocabulary_size)\n",
    "    #State saving across unrollings\n",
    "    saved_state=tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_output=tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) \n",
    "    state=saved_state\n",
    "    output=saved_output\n",
    "    \n",
    "    \n",
    "    reset_state = tf.group(\n",
    "        output.assign(tf.zeros([batch_size, num_nodes])),\n",
    "        state.assign(tf.zeros([batch_size, num_nodes])),\n",
    "        )\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    #Define our train input, decoder output variables\n",
    "    train_inputs=[]\n",
    "    decoder_inputs=[]\n",
    "    outputs=[]\n",
    "    \n",
    "    for i in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.float32,shape=[batch_size,vocabulary_size]))\n",
    "        decoder_inputs.append(tf.placeholder(tf.float32,shape=[batch_size,vocabulary_size]))\n",
    "        \n",
    "        \n",
    "    #Function Definition\n",
    "    #The Encoder function\n",
    "    def encoder(train_input,output,state):\n",
    "        '''\n",
    "        Args:       \n",
    "        \n",
    "        train_input : array of size num_unrolling, each array element is a Tensor of dimension batch_size,\n",
    "        vocabulary size.\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        output : Output of LSTM aka Hidden State\n",
    "        state : Cell state of the LSTM\n",
    "        \n",
    "        '''\n",
    "        i = len(train_inputs) - 1\n",
    "        while i >= 0:\n",
    "            output, state = encoder_lstm.lstm_cell(train_input[i],output,state)\n",
    "            i=i-1\n",
    "        #Return the last output of the lstm cell for decoding\n",
    "        return output ,state\n",
    "    \n",
    "  \n",
    "\n",
    "    def training_decoder(decoder_input,output,state):\n",
    "        outputs=[]\n",
    "        #Predict the first character using the EOS Tag. We use EOS tag as the start tag\n",
    "        output, state = decoder_lstm.lstm_cell(decoder_input[-1],output,state)\n",
    "        outputs.append(output)\n",
    "        #Now predict the next outputs using the training labels itself. Using y(n-1) to predict y(n)\n",
    "        for i in decoder_input[0:-1]:\n",
    "            output,state=decoder_lstm.lstm_cell(i,output,state)\n",
    "            outputs.append(output)\n",
    "        return outputs,output,state\n",
    "    \n",
    "    \n",
    "    def inference_decoder(go_char,decode_steps,output,state):\n",
    "        outputs=[]\n",
    "        #First input to decoder is the the Go Character\n",
    "        output,state=decoder_lstm.lstm_cell(go_char,output,state)\n",
    "        outputs.append(output)\n",
    "        for i in range(decode_steps-1):\n",
    "            #Feed the previous output as the next decoder input\n",
    "            decoder_input=tf.nn.softmax(tf.nn.xw_plus_b(output, w, b))\n",
    "            output,state=decoder_lstm.lstm_cell(decoder_input,output,state)\n",
    "            outputs.append(output)\n",
    "        return outputs,output,state\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    #Model Definition\n",
    "\n",
    "    output,state=encoder(train_inputs,output,state)\n",
    "\n",
    "    outputs,output,state=training_decoder(decoder_inputs,output,state)\n",
    "    \n",
    "\n",
    "\n",
    "    with tf.control_dependencies([saved_state.assign(state),\n",
    "                                saved_output.assign(output),\n",
    "                                    ]):\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                    labels=tf.concat(decoder_inputs, 0), logits=logits))\n",
    "        \n",
    "    #Loss function and optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                            10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "                    zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Sample Prediction\n",
    "    sample_input=[]\n",
    "    sample_outputs=[]\n",
    "\n",
    "    for i in range(num_unrollings):\n",
    "        sample_input.append(tf.placeholder(tf.float32,shape=[1,vocabulary_size]))\n",
    "        \n",
    "    sample_saved_state=tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "    sample_saved_output=tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "    \n",
    "\n",
    "    \n",
    "    sample_output=sample_saved_output\n",
    "    sample_state=sample_saved_state\n",
    "\n",
    "    \n",
    "    \n",
    "    reset_sample_state = tf.group(\n",
    "        sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        sample_state.assign(tf.zeros([1, num_nodes])),\n",
    "\n",
    "        )\n",
    "    \n",
    "\n",
    "    sample_output,sample_state=encoder(sample_input,sample_output,sample_state)\n",
    "    sample_decoder_outputs,sample_output,sample_state=inference_decoder(sample_input[-1],num_unrollings,sample_output,sample_state)\n",
    "\n",
    "    with tf.control_dependencies([sample_saved_output.assign(sample_output),\n",
    "                                sample_saved_state.assign(sample_state),\n",
    "                               ]):\n",
    "        for d in sample_decoder_outputs:\n",
    "                sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(d, w, b))\n",
    "                sample_outputs.append(sample_prediction)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "--------------------------------------------------------------------------------\n",
      "Step 0\n",
      "Loss 3.35144\n",
      "Batch Accuracy: 10.94\n",
      "Validation Accuracy: 12.98\n",
      "Input Test String ['f what are re#']\n",
      "Output Prediction[' a  a  a  a  a']\n",
      "Actual['f tahw era er#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 1000\n",
      "Loss 2.24337\n",
      "Batch Accuracy: 34.15\n",
      "Validation Accuracy: 21.13\n",
      "Input Test String ['ilism or anom#']\n",
      "Output Prediction['e          es#']\n",
      "Actual['msili ro mona#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 2000\n",
      "Loss 2.00187\n",
      "Batch Accuracy: 40.07\n",
      "Validation Accuracy: 21.13\n",
      "Input Test String ['the state the#']\n",
      "Output Prediction['e t e e e e  #']\n",
      "Actual['eht etats eht#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 3000\n",
      "Loss 1.77584\n",
      "Batch Accuracy: 47.32\n",
      "Validation Accuracy: 22.84\n",
      "Input Test String ['nts that advo#']\n",
      "Output Prediction['sn n e e a es#']\n",
      "Actual['stn taht ovda#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 4000\n",
      "Loss 1.19361\n",
      "Batch Accuracy: 64.17\n",
      "Validation Accuracy: 32.49\n",
      "Input Test String ['rpretations o#']\n",
      "Output Prediction['rritirrrrr nd#']\n",
      "Actual['snoitaterpr o#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 5000\n",
      "Loss 0.794631\n",
      "Batch Accuracy: 75.45\n",
      "Validation Accuracy: 37.22\n",
      "Input Test String ['ers are unnec#']\n",
      "Output Prediction['rer seu eeuns#']\n",
      "Actual['sre era cennu#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 6000\n",
      "Loss 0.608197\n",
      "Batch Accuracy: 81.70\n",
      "Validation Accuracy: 45.77\n",
      "Input Test String ['s ruler chief#']\n",
      "Output Prediction['r reruh ecuhc#']\n",
      "Actual['s relur feihc#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 7000\n",
      "Loss 0.418539\n",
      "Batch Accuracy: 86.05\n",
      "Validation Accuracy: 56.74\n",
      "Input Test String ['efined anarch#']\n",
      "Output Prediction['denecn yarehc#']\n",
      "Actual['denife hcrana#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 8000\n",
      "Loss 0.274119\n",
      "Batch Accuracy: 91.63\n",
      "Validation Accuracy: 65.79\n",
      "Input Test String ['ganization of#']\n",
      "Output Prediction['noitazinaz fo#']\n",
      "Actual['noitazinag fo#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 9000\n",
      "Loss 0.169569\n",
      "Batch Accuracy: 95.20\n",
      "Validation Accuracy: 70.02\n",
      "Input Test String [' pejorative w#']\n",
      "Output Prediction[' evitarepor w#']\n",
      "Actual[' evitarojep w#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 10000\n",
      "Loss 0.143137\n",
      "Batch Accuracy: 95.54\n",
      "Validation Accuracy: 77.67\n",
      "Input Test String ['the sans culo#']\n",
      "Output Prediction['eht sna solu #']\n",
      "Actual['eht snas oluc#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 11000\n",
      "Loss 0.085598\n",
      "Batch Accuracy: 97.54\n",
      "Validation Accuracy: 81.19\n",
      "Input Test String ['y working cla#']\n",
      "Output Prediction['y gnikrow alc#']\n",
      "Actual['y gnikrow alc#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 12000\n",
      "Loss 0.0814877\n",
      "Batch Accuracy: 97.66\n",
      "Validation Accuracy: 89.54\n",
      "Input Test String ['mic instituti#']\n",
      "Output Prediction['cim itutitini#']\n",
      "Actual['cim itutitsni#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 13000\n",
      "Loss 0.0832866\n",
      "Batch Accuracy: 97.77\n",
      "Validation Accuracy: 90.34\n",
      "Input Test String ['of what are r#']\n",
      "Output Prediction['fo tahw era r#']\n",
      "Actual['fo tahw era r#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 14000\n",
      "Loss 0.0636092\n",
      "Batch Accuracy: 97.88\n",
      "Validation Accuracy: 92.25\n",
      "Input Test String ['hilism or ano#']\n",
      "Output Prediction['msilih ro ona#']\n",
      "Actual['msilih ro ona#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 15000\n",
      "Loss 0.0678261\n",
      "Batch Accuracy: 97.43\n",
      "Validation Accuracy: 94.16\n",
      "Input Test String [' the state th#']\n",
      "Output Prediction[' eht etats ht#']\n",
      "Actual[' eht etats ht#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 16000\n",
      "Loss 0.0318983\n",
      "Batch Accuracy: 98.88\n",
      "Validation Accuracy: 95.27\n",
      "Input Test String ['ents that adv#']\n",
      "Output Prediction['stne taht vda#']\n",
      "Actual['stne taht vda#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 17000\n",
      "Loss 0.0319762\n",
      "Batch Accuracy: 99.33\n",
      "Validation Accuracy: 96.68\n",
      "Input Test String ['erpretations #']\n",
      "Output Prediction['snoitatrerpm #']\n",
      "Actual['snoitaterpre #']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 18000\n",
      "Loss 0.0275212\n",
      "Batch Accuracy: 99.11\n",
      "Validation Accuracy: 95.17\n",
      "Input Test String ['lers are unne#']\n",
      "Output Prediction['srel era ennu#']\n",
      "Actual['srel era ennu#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 19000\n",
      "Loss 0.0170464\n",
      "Batch Accuracy: 99.67\n",
      "Validation Accuracy: 95.67\n",
      "Input Test String ['ns ruler chie#']\n",
      "Output Prediction['sn relur eihc#']\n",
      "Actual['sn relur eihc#']\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20000\n",
    "summary_frequency = 1000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  global dropout\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches,output_batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    dropout=0.5\n",
    "    \n",
    "    for i in range(num_unrollings):\n",
    "        #Feeding input from reverse according to https://arxiv.org/abs/1409.3215\n",
    "        feed_dict[train_inputs[i]]=batches[i]\n",
    "        feed_dict[decoder_inputs[i]]=output_batches[i]\n",
    "\n",
    "        \n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    #reset_state.run()\n",
    "\n",
    "    if step % (summary_frequency ) == 0:\n",
    "        dropout=1\n",
    "        print('-'*80)\n",
    "        print('Step '+str(step))\n",
    "        print('Loss '+str(l))\n",
    "        \n",
    "        labels=np.concatenate(list(output_batches)[:])\n",
    "#        print(characters(labels))\n",
    "#       print(characters(predictions))\n",
    "        print('Batch Accuracy: %.2f' % float(accuracy(labels,predictions)*100))\n",
    "        num_validation = valid_size // num_unrollings\n",
    "        reset_sample_state.run()\n",
    "        sum_acc=0\n",
    "        for _ in range(num_validation):\n",
    "            valid,valid_output=valid_batches.next()\n",
    "            valid_feed_dict=dict()\n",
    "            for i in range(num_unrollings):\n",
    "                valid_feed_dict[sample_input[i]]=valid[i]\n",
    "            sample_pred=session.run(sample_outputs,feed_dict=valid_feed_dict)\n",
    "            labels=np.concatenate(list(valid_output)[:],axis=0)\n",
    "            pred=np.concatenate(list(sample_pred)[:],axis=0)\n",
    "            sum_acc=sum_acc+accuracy(labels,pred)\n",
    "        val_acc=sum_acc/num_validation\n",
    "        print('Validation Accuracy: %0.2f'%(val_acc*100))\n",
    "        print('Input Test String '+str(batches2string(valid)))\n",
    "        print('Output Prediction'+str(batches2string(sample_pred)))\n",
    "        print('Actual'+str(batches2string(valid_output)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism\n",
    "\n",
    "Normal Encoder Decoder architectures only look  at the fixed vector representation as provided by the Encoder. Adding an attention mechanism helps the decoder to focus on the hidden states produced at different timestep by the encoder, and only pay attention to those that are relevant at a particular decoding step.\n",
    "\n",
    "This notebook implements a soft attention. The implementation has been done as per the below diagram. Refer to https://arxiv.org/pdf/1409.0473.pdf and https://blog.heuritech.com/2016/01/20/attention-mechanism/ for more details.\n",
    "\n",
    "![alt text](https://heuritech.files.wordpress.com/2016/01/detail_attentionmodel_dotproduct1.png \"Attention Mechanism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #Variables and placeholders\n",
    "    #\n",
    "    #Weights for the hidden states accross time\n",
    "    attn_weights=tf.Variable(tf.truncated_normal([num_nodes], -0.1, 0.1))\n",
    "    #Weights for the context(hidden state) at time t-1\n",
    "    prev_hidden_weights=tf.Variable(tf.truncated_normal([num_nodes], -0.1, 0.1))\n",
    "    #LSTM for encoder and decoder\n",
    "    encoder_lstm=lstm(vocabulary_size)\n",
    "    #feed decoder Y(t-1) and attention context\n",
    "    decoder_lstm=lstm(num_nodes+vocabulary_size)\n",
    "\n",
    "    #State saving across unrollings\n",
    "    saved_state=tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_output=tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) \n",
    "    state=saved_state\n",
    "    output=saved_output\n",
    "    \n",
    "    \n",
    "    reset_state = tf.group(\n",
    "        output.assign(tf.zeros([batch_size, num_nodes])),\n",
    "        state.assign(tf.zeros([batch_size, num_nodes])),\n",
    "        )\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    #Define our train input, decoder output variables\n",
    "    train_inputs=[]\n",
    "    decoder_inputs=[]\n",
    "    outputs=[]\n",
    "    \n",
    "    for i in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.float32,shape=[batch_size,vocabulary_size]))\n",
    "        decoder_inputs.append(tf.placeholder(tf.float32,shape=[batch_size,vocabulary_size]))\n",
    "        \n",
    "        \n",
    "    #    \n",
    "    #Encoder, Decoder and attention functions\n",
    "    #\n",
    "    \n",
    "    def encoder(train_input,output,state):\n",
    "        '''\n",
    "        Args:       \n",
    "        \n",
    "        train_input : array of size num_unrolling, each array element is a Tensor of dimension batch_size,\n",
    "        vocabulary size.\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        output : Output of LSTM aka Hidden State\n",
    "        state : Cell state of the LSTM\n",
    "        \n",
    "        '''\n",
    "        i = len(train_inputs) - 1\n",
    "        outputs=[]\n",
    "        while i >= 0:\n",
    "            output, state = encoder_lstm.lstm_cell(train_input[i],output,state)\n",
    "            outputs.append(output)\n",
    "            i=i-1\n",
    "        #Return the all the outputs because they will be required by the attention mechanism\n",
    "        return outputs,output,state\n",
    "    \n",
    "    def soft_attention(hidden_states,prev_hidden_state,batch_size):\n",
    "        '''\n",
    "        \n",
    "        Implements soft attention mechanism over an array of encoder hidden \n",
    "        states given previous decoder hidden states\n",
    "        \n",
    "        Returns a context by attending over the hidden states accross time\n",
    "        \n",
    "        Used by the decoder at each timestep during decoding\n",
    "        \n",
    "        '''\n",
    "        #Prev hidden weights\n",
    "        prev_hidden_state_times_w=tf.multiply(prev_hidden_state,prev_hidden_weights)\n",
    "        for h in range(num_unrollings):\n",
    "            hidden_states[h]=tf.multiply(hidden_states[h],attn_weights)+prev_hidden_state_times_w \n",
    "        unrol_states=tf.reshape(tf.concat(hidden_states,1),(batch_size,num_unrollings,num_nodes))\n",
    "        eij=tf.tanh(unrol_states)\n",
    "        #Softmax across the unrolling dimension\n",
    "        softmax=tf.nn.softmax(eij,dim=1)\n",
    "        context=tf.reduce_sum(tf.multiply(softmax,unrol_states),axis=1) #Sum across axis time\n",
    "        return context\n",
    "        \n",
    "    \n",
    "    def training_decoder(decoder_input,hidden_states,output,state):\n",
    "        outputs=[]\n",
    "        #Predict the first character using the EOS Tag. We use EOS tag as the start tag\n",
    "        context=soft_attention(hidden_states,output,batch_size)\n",
    "        inp_concat=tf.concat([decoder_input[-1],context],axis=1)\n",
    "        output, state = decoder_lstm.lstm_cell(inp_concat,output,state)\n",
    "        outputs.append(output)\n",
    "        #Now predict the next outputs using the training labels itself. Using y(n-1) to predict y(n)\n",
    "        for i in decoder_input[0:-1]:\n",
    "            context=soft_attention(hidden_states,output,batch_size)\n",
    "            inp_concat=tf.concat([i,context],axis=1)\n",
    "            output,state=decoder_lstm.lstm_cell(inp_concat,output,state)\n",
    "            outputs.append(output)\n",
    "            \n",
    "        return outputs,output,state\n",
    "    \n",
    "    \n",
    "    def inference_decoder(go_char,hidden_states,decode_steps,output,state):\n",
    "        outputs=[]\n",
    "        #First input to decoder is the the Go Character\n",
    "        context=soft_attention(hidden_states,output,1)\n",
    "        inp_concat=tf.concat([go_char,context],axis=1)\n",
    "        output,state=decoder_lstm.lstm_cell(inp_concat,output,state)\n",
    "        outputs.append(output)\n",
    "        for i in range(decode_steps-1):\n",
    "            #Feed the previous output as the next decoder input\n",
    "            decoder_input=tf.nn.softmax(tf.nn.xw_plus_b(output, w, b))\n",
    "            context=soft_attention(hidden_states,output,1)\n",
    "            inp_concat=tf.concat([decoder_input,context],axis=1)\n",
    "            output,state=decoder_lstm.lstm_cell(inp_concat,output,state)\n",
    "            outputs.append(output)\n",
    "        return outputs,output,state\n",
    "    \n",
    "\n",
    "        \n",
    "    #\n",
    "    #Model Definition\n",
    "    #\n",
    "    hidden_states,output,state=encoder(train_inputs,output,state)\n",
    "    outputs,output,state=training_decoder(decoder_inputs,hidden_states,output,state)\n",
    "    \n",
    "\n",
    "\n",
    "    with tf.control_dependencies([saved_state.assign(state),\n",
    "                                saved_output.assign(output),\n",
    "                                    ]):\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                    labels=tf.concat(decoder_inputs, 0), logits=logits))\n",
    "        \n",
    "    #Loss function and optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                            10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "                    zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    #  \n",
    "    #Sample Prediction\n",
    "    #\n",
    "    sample_input=[]\n",
    "    sample_outputs=[]\n",
    "\n",
    "    for i in range(num_unrollings):\n",
    "        sample_input.append(tf.placeholder(tf.float32,shape=[1,vocabulary_size]))\n",
    "        \n",
    "    sample_saved_state=tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "    sample_saved_output=tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "    \n",
    "\n",
    "    \n",
    "    sample_output=sample_saved_output\n",
    "    sample_state=sample_saved_state\n",
    "\n",
    "    \n",
    "    \n",
    "    reset_sample_state = tf.group(\n",
    "        sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        sample_state.assign(tf.zeros([1, num_nodes])),\n",
    "\n",
    "        )\n",
    "    \n",
    "\n",
    "    hidden_states,sample_output,sample_state=encoder(sample_input,sample_output,sample_state)\n",
    "    sample_decoder_outputs,sample_output,sample_state=inference_decoder(sample_input[-1],hidden_states,num_unrollings,sample_output,sample_state)\n",
    "\n",
    "    with tf.control_dependencies([sample_saved_output.assign(sample_output),\n",
    "                                sample_saved_state.assign(sample_state),\n",
    "                               ]):\n",
    "        for d in sample_decoder_outputs:\n",
    "                sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(d, w, b))\n",
    "                sample_outputs.append(sample_prediction)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "--------------------------------------------------------------------------------\n",
      "Step 0\n",
      "Loss 3.36606\n",
      "Batch Accuracy: 5.69\n",
      "Validation Accuracy: 7.65\n",
      "Input Test String ['defined anarc#']\n",
      "Output Prediction[' ez ez ezeezee']\n",
      "Actual['denifed crana#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 1000\n",
      "Loss 2.35367\n",
      "Batch Accuracy: 32.03\n",
      "Validation Accuracy: 20.52\n",
      "Input Test String ['rganization o#']\n",
      "Output Prediction['  i     e  e #']\n",
      "Actual['noitazinagr o#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 2000\n",
      "Loss 2.00279\n",
      "Batch Accuracy: 38.73\n",
      "Validation Accuracy: 20.32\n",
      "Input Test String ['a pejorative #']\n",
      "Output Prediction['seit ee eesep#']\n",
      "Actual['a evitarojep #']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 3000\n",
      "Loss 1.76183\n",
      "Batch Accuracy: 48.10\n",
      "Validation Accuracy: 24.85\n",
      "Input Test String [' the sans cul#']\n",
      "Output Prediction[' eit eieee o #']\n",
      "Actual[' eht snas luc#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 4000\n",
      "Loss 1.53063\n",
      "Batch Accuracy: 54.58\n",
      "Validation Accuracy: 25.96\n",
      "Input Test String ['ly working cl#']\n",
      "Output Prediction['yl dlololoood#']\n",
      "Actual['yl gnikrow lc#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 5000\n",
      "Loss 0.717488\n",
      "Batch Accuracy: 78.24\n",
      "Validation Accuracy: 48.19\n",
      "Input Test String ['omic institut#']\n",
      "Output Prediction['cici ttitimoc#']\n",
      "Actual['cimo tutitsni#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 6000\n",
      "Loss 0.319305\n",
      "Batch Accuracy: 90.40\n",
      "Validation Accuracy: 64.79\n",
      "Input Test String [' of what are #']\n",
      "Output Prediction[' fo aawt eh h#']\n",
      "Actual[' fo tahw era #']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 7000\n",
      "Loss 0.206271\n",
      "Batch Accuracy: 93.75\n",
      "Validation Accuracy: 74.04\n",
      "Input Test String ['ihilism or an#']\n",
      "Output Prediction['sihirim ro na#']\n",
      "Actual['msilihi ro na#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 8000\n",
      "Loss 0.164243\n",
      "Batch Accuracy: 94.20\n",
      "Validation Accuracy: 82.90\n",
      "Input Test String ['y the state t#']\n",
      "Output Prediction['y eht etats t#']\n",
      "Actual['y eht etats t#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 9000\n",
      "Loss 0.0772509\n",
      "Batch Accuracy: 97.77\n",
      "Validation Accuracy: 90.14\n",
      "Input Test String ['ments that ad#']\n",
      "Output Prediction['stnem taht da#']\n",
      "Actual['stnem taht da#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 10000\n",
      "Loss 0.0800756\n",
      "Batch Accuracy: 97.54\n",
      "Validation Accuracy: 90.24\n",
      "Input Test String ['terpretations#']\n",
      "Output Prediction['snoitaterpper#']\n",
      "Actual['snoitaterpret#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 11000\n",
      "Loss 0.0687864\n",
      "Batch Accuracy: 97.77\n",
      "Validation Accuracy: 92.96\n",
      "Input Test String ['ulers are unn#']\n",
      "Output Prediction['srelu era nun#']\n",
      "Actual['srelu era nnu#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 12000\n",
      "Loss 0.0652413\n",
      "Batch Accuracy: 97.88\n",
      "Validation Accuracy: 94.06\n",
      "Input Test String ['ons ruler chi#']\n",
      "Output Prediction['sno relur ihc#']\n",
      "Actual['sno relur ihc#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 13000\n",
      "Loss 0.0486363\n",
      "Batch Accuracy: 98.44\n",
      "Validation Accuracy: 95.37\n",
      "Input Test String [' defined anar#']\n",
      "Output Prediction[' denifed rana#']\n",
      "Actual[' denifed rana#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 14000\n",
      "Loss 0.0318363\n",
      "Batch Accuracy: 99.22\n",
      "Validation Accuracy: 95.77\n",
      "Input Test String ['organization #']\n",
      "Output Prediction['noitazinagro #']\n",
      "Actual['noitazinagro #']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 15000\n",
      "Loss 0.0307838\n",
      "Batch Accuracy: 99.44\n",
      "Validation Accuracy: 96.08\n",
      "Input Test String [' a pejorative#']\n",
      "Output Prediction[' a evitaroper#']\n",
      "Actual[' a evitarojep#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 16000\n",
      "Loss 0.0251969\n",
      "Batch Accuracy: 99.33\n",
      "Validation Accuracy: 98.19\n",
      "Input Test String ['d the sans cu#']\n",
      "Output Prediction['d eht snas uc#']\n",
      "Actual['d eht snas uc#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 17000\n",
      "Loss 0.0207135\n",
      "Batch Accuracy: 99.44\n",
      "Validation Accuracy: 96.78\n",
      "Input Test String ['rly working c#']\n",
      "Output Prediction['ylr gnikrow c#']\n",
      "Actual['ylr gnikrow c#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 18000\n",
      "Loss 0.0213295\n",
      "Batch Accuracy: 99.33\n",
      "Validation Accuracy: 96.58\n",
      "Input Test String ['nomic institu#']\n",
      "Output Prediction['cimon utitsni#']\n",
      "Actual['cimon utitsni#']\n",
      "--------------------------------------------------------------------------------\n",
      "Step 19000\n",
      "Loss 0.0244272\n",
      "Batch Accuracy: 99.11\n",
      "Validation Accuracy: 98.49\n",
      "Input Test String ['e of what are#']\n",
      "Output Prediction['e fo tahw are#']\n",
      "Actual['e fo tahw era#']\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20000\n",
    "summary_frequency = 1000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  global dropout\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches,output_batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    dropout=0.5\n",
    "    \n",
    "    for i in range(num_unrollings):\n",
    "        #Feeding input from reverse according to https://arxiv.org/abs/1409.3215\n",
    "        feed_dict[train_inputs[i]]=batches[i]\n",
    "        feed_dict[decoder_inputs[i]]=output_batches[i]\n",
    "\n",
    "        \n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    #reset_state.run()\n",
    "\n",
    "    if step % (summary_frequency ) == 0:\n",
    "        dropout=1\n",
    "        print('-'*80)\n",
    "        print('Step '+str(step))\n",
    "        print('Loss '+str(l))\n",
    "        \n",
    "        labels=np.concatenate(list(output_batches)[:])\n",
    "#        print(characters(labels))\n",
    "#       print(characters(predictions))\n",
    "        print('Batch Accuracy: %.2f' % float(accuracy(labels,predictions)*100))\n",
    "        \n",
    "        num_validation = valid_size // num_unrollings\n",
    "        reset_sample_state.run()\n",
    "        sum_acc=0\n",
    "        for _ in range(num_validation):\n",
    "            valid,valid_output=valid_batches.next()\n",
    "            valid_feed_dict=dict()\n",
    "            for i in range(num_unrollings):\n",
    "                valid_feed_dict[sample_input[i]]=valid[i]\n",
    "            sample_pred=session.run(sample_outputs,feed_dict=valid_feed_dict)\n",
    "            labels=np.concatenate(list(valid_output)[:],axis=0)\n",
    "            pred=np.concatenate(list(sample_pred)[:],axis=0)\n",
    "            sum_acc=sum_acc+accuracy(labels,pred)\n",
    "        val_acc=sum_acc/num_validation\n",
    "        print('Validation Accuracy: %0.2f'%(val_acc*100))\n",
    "        print('Input Test String '+str(batches2string(valid)))\n",
    "        print('Output Prediction'+str(batches2string(sample_pred)))\n",
    "        print('Actual'+str(batches2string(valid_output)))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
